{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch as tr\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import pathlib\n",
    "import tqdm\n",
    "from tqdm import trange"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tinyshakespeare = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "response = requests.get(url=tinyshakespeare)\n",
    "\n",
    "with open('./data/tinyshakespeare.txt', mode='w') as txt:\n",
    "    txt.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us\n"
     ]
    }
   ],
   "source": [
    "text_file = pathlib.Path('./data/tinyshakespeare.txt')\n",
    "text_file = text_file.read_text()\n",
    "\n",
    "print(text_file[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters : \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size 65\n"
     ]
    }
   ],
   "source": [
    "all_chars = sorted(list(set(text_file)))\n",
    "vocab_size = len(all_chars)\n",
    "print('Characters :',''.join(all_chars))\n",
    "print('Vocab size', vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a mapping from characters to integers and vice-versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers and vice-versa\n",
    "\n",
    "char_to_id = tf.keras.layers.StringLookup(vocabulary=all_chars, mask_token=None)\n",
    "id_to_char = tf.keras.layers.StringLookup(vocabulary=char_to_id.get_vocabulary(), invert=True, mask_token=None)\n",
    "id_to_str = lambda id : tf.strings.reduce_join(inputs=id_to_char(id), axis=-1, separator='')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = char_to_id(tf.strings.unicode_split(text_file, input_encoding='UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1], dtype=int64)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_and_target(text):\n",
    "    input = text[:-1]\n",
    "    target = text[1:]\n",
    "    return input, target\n",
    "\n",
    "seq_len = 101\n",
    "text_ds = (tf.data.Dataset\n",
    "           .from_tensor_slices(all_ids)\n",
    "           .batch(batch_size=seq_len, drop_remainder=True)\n",
    "           .map(input_and_target)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch, target_batch = next(iter(text_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(id_to_str(input_batch).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n"
     ]
    }
   ],
   "source": [
    "print(id_to_str(target_batch).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (text_ds\n",
    "           .shuffle(BUFFER_SIZE)\n",
    "           .batch(BATCH_SIZE, drop_remainder=True)\n",
    "           .prefetch(tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sample_inputs, sample_targets) = next(iter(dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Character Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "    \n",
    "    def call(self, query, keys, values, mask=None):\n",
    "\n",
    "        kdims = tf.cast(keys.shape[-1], dtype=tf.float32)\n",
    "\n",
    "        # [batch, query, dims] @ [batch, keys, dims]^T --> [batch, query, keys]\n",
    "        prod = tf.matmul(query, keys, transpose_b=True)\n",
    "        # scaling\n",
    "        prod_scaled = prod / tf.math.sqrt(kdims)\n",
    "\n",
    "        # apply mask\n",
    "        if mask is not None:\n",
    "            prod_scaled += (mask*-1e9)\n",
    "        \n",
    "        weights = tf.nn.softmax(prod_scaled, axis=-1)\n",
    "\n",
    "        # [batch, query, key] @ [batch, value, dim] --> [batch, query, dim]\n",
    "        attention = tf.matmul(weights, values)\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tokens : (64, 100)\n",
      "Shape of query Embedding : (64, 100, 512)\n",
      "Shape of query Embedding : (64, 100, 512)\n",
      "Shape of query Embedding : (64, 100, 512)\n",
      "Shape of Attention output : (64, 100, 512)\n"
     ]
    }
   ],
   "source": [
    "# Lets test the attention \n",
    "emb = tf.keras.layers.Embedding(input_dim=char_to_id.vocabulary_size(), output_dim=512)\n",
    "sample_emb = emb(sample_inputs)\n",
    "q,k,v = [sample_emb]*3\n",
    "\n",
    "attention = Attention()\n",
    "attention_output = attention(q,k,v,mask=None)\n",
    "print('Shape of input tokens :',sample_inputs.shape)\n",
    "print('Shape of query Embedding :',q.shape)\n",
    "print('Shape of query Embedding :',k.shape)\n",
    "print('Shape of query Embedding :',v.shape)\n",
    "print('Shape of Attention output :',attention_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, dims, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.dims=dims\n",
    "        self.num_heads=num_heads\n",
    "        self.head_dims=dims//num_heads\n",
    "\n",
    "        tf.assert_equal(x=dims%num_heads, y=0, message='MultiHeadAttention')\n",
    "\n",
    "        self.dq = tf.keras.layers.Dense(dims)\n",
    "        self.dk = tf.keras.layers.Dense(dims)\n",
    "        self.dv = tf.keras.layers.Dense(dims)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(dims)\n",
    "\n",
    "        self.attention = Attention()\n",
    "\n",
    "    def split_head(self, vector):\n",
    "        # [batch, seq, dim] --> [batch, seq, num_head, head_dims]\n",
    "        vector = tf.reshape(vector, shape=[self.batch_dims, -1, self.num_heads, self.head_dims])\n",
    "        # [batch, seq, num_head, head_dims] --> [batch, num_head, seq, head_dims]\n",
    "        vector = tf.transpose(vector, perm=[0, 2, 1, 3])\n",
    "        return vector\n",
    "    \n",
    "    def concat_heads(self, vector):\n",
    "        # [batch, num_head, seq, dims] --> [batch, seq, num_head, dims]\n",
    "        vector = tf.transpose(vector, perm=[0, 2, 1, 3])\n",
    "        # [batch, seq, num_head, dims] --> [batch, seq, dim]\n",
    "        vector = tf.reshape(vector, shape=[self.batch_dims, -1, self.num_heads*self.head_dims])\n",
    "        return vector\n",
    "\n",
    "\n",
    "    def call(self, q, k, v, mask):\n",
    "\n",
    "        self.batch_dims = tf.shape(q)[0]\n",
    "\n",
    "        q = self.dq(q)\n",
    "        k = self.dk(k)\n",
    "        v = self.dv(v)\n",
    "\n",
    "        # multi head --> [batch, num_head, seq, dims]\n",
    "        q = self.split_head(q)\n",
    "        k = self.split_head(k)\n",
    "        v = self.split_head(v)\n",
    "\n",
    "        # ATTENTION --> [batch, num_head, seq, dims]\n",
    "        self_attention = self.attention(q, k, v, mask)\n",
    "\n",
    "        # concat heads [batch, num_head, seq, dims] --> [batch, seq, dim]\n",
    "        self_attention = self.concat_heads(self_attention)\n",
    "\n",
    "        # Projection --> [batch, seq, dim]\n",
    "        return self.dense(self_attention)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tokens : (64, 100)\n",
      "Shape of query Embedding : (64, 100, 512)\n",
      "Shape of query Embedding : (64, 100, 512)\n",
      "Shape of query Embedding : (64, 100, 512)\n",
      "Shape of Multi Head Attention output : (64, 100, 512)\n"
     ]
    }
   ],
   "source": [
    "# Lets test Multi Head Attention\n",
    "mha = MultiHeadAttention(dims=512, num_heads=8)\n",
    "\n",
    "multi_head_attention_output = mha(q, k ,v, mask=None)\n",
    "\n",
    "print('Shape of input tokens :', sample_inputs.shape)\n",
    "print('Shape of query Embedding :',q.shape)\n",
    "print('Shape of query Embedding :',k.shape)\n",
    "print('Shape of query Embedding :',v.shape)\n",
    "print('Shape of Multi Head Attention output :',multi_head_attention_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(tf.keras.layers.Layer):\n",
    "    def __init__(self, ffnn_units, dims):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.ffnn=tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(units=ffnn_units, activation='relu'),\n",
    "            tf.keras.layers.Dense(units=dims)\n",
    "        ])\n",
    "    def call(self, inputs):\n",
    "        return self.ffnn(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tokens : (64, 100)\n",
      "Shape of query Embedding : (64, 100, 512)\n",
      "Shape of query Embedding : (64, 100, 512)\n",
      "Shape of query Embedding : (64, 100, 512)\n",
      "Shape of Multi Head Attention output : (64, 100, 512)\n",
      "Shape of Feed Forward Neural Nets output : (64, 100, 512)\n"
     ]
    }
   ],
   "source": [
    "# lets test FFNN\n",
    "ffnn = FFNN(ffnn_units=2048, dims=512)\n",
    "ffnn_output = ffnn(multi_head_attention_output)\n",
    "\n",
    "print('Shape of input tokens :',sample_inputs.shape)\n",
    "print('Shape of query Embedding :',q.shape)\n",
    "print('Shape of query Embedding :',k.shape)\n",
    "print('Shape of query Embedding :',v.shape)\n",
    "print('Shape of Multi Head Attention output :', multi_head_attention_output.shape)\n",
    "print('Shape of Feed Forward Neural Nets output :', ffnn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAUSAL MASK\n",
      "tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]], shape=(8, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def causal_mask(size):\n",
    "    ones = tf.ones(shape=[size,size])\n",
    "    # triangular matrix ---> [size, size]\n",
    "    lt = tf.linalg.band_part(input=ones, num_lower=-1, num_upper=0)\n",
    "    return 1 - lt\n",
    "\n",
    "# lets test the causal_mask \n",
    "mask = causal_mask(8)\n",
    "print('CAUSAL MASK')\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, dims, num_heads, ffnn_units, dropout_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.causal_multi_head_attention = MultiHeadAttention(dims=dims, num_heads=num_heads)\n",
    "        self.ffnn = FFNN(dims=dims, ffnn_units=ffnn_units)\n",
    "\n",
    "        self.add_and_norm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.add_and_norm_2 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "\n",
    "        self.drop_out_1 = tf.keras.layers.Dropout(rate=dropout_rate)\n",
    "        self.drop_out_2 = tf.keras.layers.Dropout(rate=dropout_rate)\n",
    "\n",
    "\n",
    "    def call(self, inputs, mask, training):\n",
    "\n",
    "        # pre-norm\n",
    "        layer_norm_1 = self.add_and_norm_1(inputs)\n",
    "\n",
    "        # self attention query, keys, values\n",
    "        q, k, v = [layer_norm_1]*3\n",
    "\n",
    "        # causal/masked multi-head-attention-layer --> [batch, seq, dims]\n",
    "        causal_attention = self.causal_multi_head_attention(q, k, v, mask)\n",
    "        \n",
    "        # drop out\n",
    "        out_1 = self.drop_out_1(causal_attention, training=training)\n",
    "\n",
    "        # skip connection\n",
    "        out_1 += inputs\n",
    "\n",
    "        # pre-norm\n",
    "        layer_norm_2 = self.add_and_norm_2(out_1)\n",
    "        # Feed Forward Neural Nets --> [batch, seq, dims]\n",
    "        fully_connected = self.ffnn(layer_norm_2)  \n",
    "        # dropout\n",
    "        out_2 = self.drop_out_2(fully_connected, training=training)\n",
    "\n",
    "        return out_2 + out_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tokens : (64, 100)\n",
      "Shape of input Embedding : (64, 100, 512)\n",
      "Decoder Layer Output  (64, 100, 512)\n"
     ]
    }
   ],
   "source": [
    "# lets test decoder layer\n",
    "decoder_layer = DecoderLayer(dims=512, num_heads=8, ffnn_units=2048, dropout_rate=0.2)\n",
    "mask = causal_mask(size=100)\n",
    "decoder_layer_output = decoder_layer(sample_emb, mask, training=True)\n",
    "\n",
    "\n",
    "print('Shape of input tokens :',sample_inputs.shape)\n",
    "print('Shape of input Embedding :',sample_emb.shape)\n",
    "print('Decoder Layer Output ',decoder_layer_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, dims, num_heads, ffnn_units, dropout_rate, n_decoder_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.decoder_layers = [\n",
    "            DecoderLayer(dims, num_heads, ffnn_units, dropout_rate) \n",
    "            for _ in range(n_decoder_layers)]\n",
    "    \n",
    "    def call(self, vectors, mask, training):\n",
    "\n",
    "        for decoder in self.decoder_layers:\n",
    "            vectors = decoder(vectors, mask, training)\n",
    "        \n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tokens : (64, 100)\n",
      "Shape of input Embedding : (64, 100, 512)\n",
      "Decoder Layer Output  (64, 100, 512)\n"
     ]
    }
   ],
   "source": [
    "# lets test decoder\n",
    "decoder = Decoder(dims=512, num_heads=8, ffnn_units=2048, dropout_rate=0.2, n_decoder_layers=6)\n",
    "mask = causal_mask(size=100)\n",
    "decoder_output = decoder(sample_emb, mask, training=True)\n",
    "\n",
    "\n",
    "print('Shape of input tokens :',sample_inputs.shape)\n",
    "print('Shape of input Embedding :',sample_emb.shape)\n",
    "print('Decoder Layer Output ',decoder_output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Positional \\: Embedding$$\n",
    "\n",
    "$$ PE_{(position, 2i)} = \\sin({\\frac{positions}{10000^{\\frac{2i}{dims}}}})$$\n",
    "$$ PE_{(position, 2i+1)} = \\cos({\\frac{positions}{10000^{\\frac{2i}{dims}}}})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angle(pos, i, dims):\n",
    "    neu = pos\n",
    "    deno = 10000**(2*(i//2) / dims)\n",
    "    angle = neu / deno\n",
    "    return angle\n",
    "    \n",
    "def position_embedding(positions, dims):\n",
    "    pos = np.arange(positions)[:, np.newaxis]\n",
    "    i = np.arange(dims)[np.newaxis,:] \n",
    "    theta = get_angle(pos, i , dims)\n",
    "\n",
    "    theta[:,0::2] = np.sin(theta[:,0::2])\n",
    "    theta[:,1::2] = np.cos(theta[:,1::2])\n",
    "\n",
    "    return tf.cast(theta[np.newaxis, :], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "tf_vocab_size = char_to_id.vocabulary_size()\n",
    "\n",
    "# The embedding dimension\n",
    "tf_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "tf_ffnn_units = 1024\n",
    "\n",
    "# Positional embedding\n",
    "tf_position = 100\n",
    "\n",
    "# Drop out rate\n",
    "tf_dropout_rate = 0.2\n",
    "\n",
    "# Attention heads\n",
    "tf_num_heads = 8\n",
    "\n",
    "# Decoder Layers\n",
    "tf_n_decoder_layers = 6\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positional embeddings are initialized with a range of -1.0 to +1.0,  but the word-embeddings are initialized with a mean of 0.0 and s.d. of embedding_dim ** -0.5 .\n",
    "\n",
    "The positional embeddings would overwhelm any signal coming from the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of PE 0.3573661 \n",
      "Std of PE 0.6101553 \n",
      "Min value of PE -1.0 \n",
      "Max value of PE 1.0\n",
      "\n",
      "Mean of Embedding layer -9.220097e-05 \n",
      "Std of embedding layer 0.028716143 \n",
      "Min of embedding layer -0.049995292 \n",
      "Max of embedding layer 0.04999883\n",
      "\n",
      "The word embeddings are scaled by math.sqrt(embed_dim) (22.6 for 512, 32 for 1024)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pe = position_embedding(100, 512)\n",
    "\n",
    "print('Mean of PE',tf.math.reduce_mean(pe).numpy(),\n",
    "       '\\nStd of PE',tf.math.reduce_std(pe).numpy(),\n",
    "       '\\nMin value of PE', np.min(pe),\n",
    "       '\\nMax value of PE', np.max(pe))\n",
    "\n",
    "\n",
    "print('\\nMean of Embedding layer',tf.math.reduce_mean(sample_emb).numpy(),\n",
    "       '\\nStd of embedding layer',tf.math.reduce_std(sample_emb).numpy(),\n",
    "       '\\nMin of embedding layer',tf.math.reduce_min(sample_emb).numpy(),\n",
    "       '\\nMax of embedding layer',tf.math.reduce_max(sample_emb).numpy(),)\n",
    "\n",
    "print('\\nThe word embeddings are scaled by math.sqrt(embed_dim) (22.6 for 512, 32 for 1024)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, positions, dims, vocab_size, ffnn_units, dropout_rate, num_heads, n_decoder_layers):\n",
    "        super(CharacterModel, self).__init__()\n",
    "        self.dims = dims\n",
    "        self.embeddings = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=dims)\n",
    "        self.pe = position_embedding(positions, dims)\n",
    "        self.causal_mask = causal_mask(size=positions)\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=dropout_rate)\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        self.dense = tf.keras.layers.Dense(units=vocab_size)\n",
    "\n",
    "        self.decoder = Decoder(dims, num_heads, ffnn_units, dropout_rate, n_decoder_layers)\n",
    "    \n",
    "\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \n",
    "        # [batch, sequence] --> [batch, sequence, embedding_dims]\n",
    "        embed = self.embeddings(inputs, training=training)\n",
    "        # positional embeddings are initialized with a range of -1.0 to +1.0, \n",
    "        # but the word-embeddings are initialized with a mean of 0.0 and s.d. of embedding_dim ** -0.5 \n",
    "        # The positional embeddings would overwhelm any signal coming from the word embeddings.\n",
    "        # To prevent that we scale embeddings by a factor of math.sqrt(embed_dims)\n",
    "        embed *= tf.math.sqrt(tf.cast(self.dims, tf.float32))\n",
    "\n",
    "        # [batch, seq, dims]\n",
    "        (batch, sequence, embedding_dims) = embed.shape\n",
    "\n",
    "        # positional embedding\n",
    "        if not training:\n",
    "            pe = self.pe[:,:sequence, :]\n",
    "        else:\n",
    "            pe = self.pe\n",
    "\n",
    "        # embeddings + positional embedding\n",
    "        # [batch, seq, dims]\n",
    "        meaning_and_order = embed + pe\n",
    "\n",
    "        # Causal mask\n",
    "        mask = self.causal_mask[:sequence,:sequence]\n",
    "        # decoder block\n",
    "        context_meaning_order = self.decoder(meaning_and_order, mask, training=training)\n",
    "\n",
    "        # layer norm\n",
    "        vector = self.layer_norm(context_meaning_order)\n",
    "\n",
    "        # dropout\n",
    "        vector = self.dropout(vector, training=training)\n",
    "\n",
    "        # logits  # un-normalized probabilities\n",
    "        logits = self.dense(vector)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, inputs):\n",
    "        X, y = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self(inputs=X, training=True)\n",
    "            loss =  self.compiled_loss(y, logits, regularization_losses=self.losses)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        self.compiled_metrics.update_state(y, logits)\n",
    "        return {m.name: m.result() for m in self.metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_model = CharacterModel(\n",
    "    positions=100, dims=512, vocab_size=id_to_char.vocabulary_size(), \n",
    "    ffnn_units=2048, dropout_rate=0.2, num_heads=8, n_decoder_layers=6)\n",
    "\n",
    "\n",
    "sample_logits = char_model(sample_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Input shape: (64, 100)\n",
      "Shape logits output: (64, 100, 66)\n"
     ]
    }
   ],
   "source": [
    "print('Shape Input shape:', sample_inputs.shape)\n",
    "print('Shape logits output:', sample_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Prediction shape (64, 1)\n",
      "b'?VWdLhW[UNK]?U;LxE;OJl,!xcrRJDhtd;CICIWjYRCw;O?j?CcC:WzCCqzxpAzZ;;t;'\n"
     ]
    }
   ],
   "source": [
    "sample_predictions = tf.random.categorical(logits=sample_logits[:,-1,:], num_samples=1)\n",
    "print('Sample Prediction shape',sample_predictions.shape)\n",
    "print(id_to_str(tf.squeeze(sample_predictions)).numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "char_model = CharacterModel(\n",
    "    positions=100, dims=32, vocab_size=id_to_char.vocabulary_size(), \n",
    "    ffnn_units=32, dropout_rate=0.2, num_heads=4, n_decoder_layers=2)\n",
    "\n",
    "\n",
    "char_model.compile(\n",
    "    optimizer='adam', \n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "150/150 [==============================] - 39s 208ms/step - loss: 3.2695 - accuracy: 0.1801 - val_loss: 2.6932 - val_accuracy: 0.2541\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 32s 203ms/step - loss: 2.6999 - accuracy: 0.2489 - val_loss: 2.5412 - val_accuracy: 0.2753\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 33s 207ms/step - loss: 2.5942 - accuracy: 0.2647 - val_loss: 2.4720 - val_accuracy: 0.2863\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 33s 205ms/step - loss: 2.5141 - accuracy: 0.2831 - val_loss: 2.3825 - val_accuracy: 0.3121\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 32s 203ms/step - loss: 2.4463 - accuracy: 0.2998 - val_loss: 2.3313 - val_accuracy: 0.3221\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 32s 204ms/step - loss: 2.4057 - accuracy: 0.3087 - val_loss: 2.3070 - val_accuracy: 0.3274\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 32s 204ms/step - loss: 2.3753 - accuracy: 0.3162 - val_loss: 2.2775 - val_accuracy: 0.3344\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 35s 219ms/step - loss: 2.3503 - accuracy: 0.3218 - val_loss: 2.2550 - val_accuracy: 0.3404\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 33s 204ms/step - loss: 2.3281 - accuracy: 0.3275 - val_loss: 2.2299 - val_accuracy: 0.3491\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 33s 207ms/step - loss: 2.3035 - accuracy: 0.3345 - val_loss: 2.2004 - val_accuracy: 0.3555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2017a53ef20>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cb = tf.keras.callbacks.ModelCheckpoint(filepath='./checkpoint/{epoch}_ckpt', save_weights_only=True)\n",
    "\n",
    "tf_val_ds = dataset.take(22)\n",
    "tf_train_ds = dataset.skip(22)\n",
    "\n",
    "char_model.fit(x=tf_train_ds, epochs=10, validation_data=tf_val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateText(tf.Module):\n",
    "    def __init__(self, model, char_to_id, id_to_char, id_to_str, seed_window_size):\n",
    "        super(GenerateText, self).__init__()\n",
    "        self.model = model\n",
    "        self.char_to_id = char_to_id\n",
    "        self.id_to_char = id_to_char\n",
    "        self.id_to_str = id_to_str\n",
    "        # Positional embedding input dims = seq_len\n",
    "        self.seed_window_size = seed_window_size\n",
    "    \n",
    "    def process_inputs(self, inputs):\n",
    "        tensor_array = tf.TensorArray(dtype=tf.int64, dynamic_size=True, size=1)\n",
    "        ragged = self.char_to_id(tf.strings.unicode_split(inputs, input_encoding='UTF-8')).to_tensor()\n",
    "        for i, tensor in enumerate(ragged):\n",
    "            tensor_array=tensor_array.write(i, tensor[-self.seed_window_size:])\n",
    "        return tensor_array.stack()\n",
    "\n",
    "    def sample(self, logits):\n",
    "        # focus on the last timestep\n",
    "        last = logits[:,-1,:]\n",
    "        return tf.random.categorical(logits=last, num_samples=1)\n",
    "    \n",
    "    def __call__(self, inputs, n_iter=1000):\n",
    "        # [batch of text] --> [batch, window]\n",
    "        input_tokens = self.process_inputs(inputs)\n",
    "        output_tokens = tf.TensorArray(dtype=tf.int64, dynamic_size=True, size=1)\n",
    "\n",
    "        for i in tf.range(n_iter):\n",
    "            # [batch, window] --> [batch, window, vocab_size]\n",
    "            logits = self.model(inputs=input_tokens, training=False)\n",
    "            # [batch, window, vocab_size] --> [batch, 1]\n",
    "            sample = self.sample(logits) \n",
    "            \n",
    "            # write sampled tokens\n",
    "            output_tokens = output_tokens.write(i, sample)\n",
    "\n",
    "            # [batch, window] --> [batch, window+1]\n",
    "            input_tokens = tf.concat(values=[input_tokens, sample], axis=1)\n",
    "            input_tokens = input_tokens[:, -self.seed_window_size:]\n",
    "\n",
    "        # [n_iter, batch, 1] --> [n_iter, batch]\n",
    "        output_tokens = output_tokens.stack()\n",
    "        output_tokens = tf.squeeze(output_tokens, axis=-1)\n",
    "        # [n_iter, batch] --> [batch, n_iter]\n",
    "        output_tokens = tf.transpose(output_tokens, perm=(1,0))\n",
    "        # text from ids\n",
    "        outputs = self.id_to_str(output_tokens)\n",
    "\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets test generating some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = GenerateText(char_model, char_to_id, id_to_char, id_to_str, seed_window_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_sample_inputs = [\n",
    "\"\"\"ROMEO:\n",
    "Why, sir, what think you, sir?,\"\"\",\n",
    "\n",
    "\"\"\"Caius Marcius is chief enemy to the people.\"\"\" ,\n",
    "\n",
    "\"\"\"All:\n",
    "No more talking on't; let it be done: away, away!\"\"\"  ,\n",
    "\n",
    "\"\"\"\n",
    "First Citizen:\n",
    "Before we proceed any further, hear me speak.\n",
    "\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_outputs = generate_text(tf.constant(['ROMEO:']), n_iter=300 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Why will hes, gor! I not yous thous rar mom nou waker! sins,\n",
      "\n",
      "Codelavels ayicpd\n",
      "Pasthe ogyses brimff mencerast bat erind,\n",
      "Whik to y Gre all cof gistis\n",
      "Wie le''gh walds hom cooce grounche, nine\n",
      "Mugh ushert shigharst ooond palle at. Coh!\n",
      "\n",
      "ARNCEBOFAN fak blos, busnfors, the youl berm,\n",
      "Ar ivotind iss d\n"
     ]
    }
   ],
   "source": [
    "print(generated_outputs.numpy()[0].decode('utf-8'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_model_new= CharacterModel(\n",
    "    positions=100, dims=256, vocab_size=id_to_char.vocabulary_size(), \n",
    "    ffnn_units=64, dropout_rate=0.2, num_heads=4, n_decoder_layers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x1ba382a6ce0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_model_new.load_weights('./tf_saved_model/char_model_d256_h4_n6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = GenerateText(char_model_new, char_to_id, id_to_char, id_to_str, seed_window_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_outputs = generate_text(tf_sample_inputs, n_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " small play them?\n",
      "\n",
      "JULIET:\n",
      "Why, I charge thee? my mother to thee:\n",
      "Scut ase the land's lady earth, whose vigours.\n",
      "\n",
      "KING EDWARD IV:\n",
      "\n",
      "Nurse:\n",
      "Nod the duke that his night, grant make so for Tybalt\n",
      "Before I come, take my liege and clear.\n",
      "\n",
      "SICINIUS:\n",
      "The king crown's the pount of my hearts\n",
      "Of him feelow win helm takes him trouble\n",
      "Aid leave us fearful to means a deceived:\n",
      "And he that I pleased hear me do and from your found,\n",
      "Yet here we are in his land? Aumerled, come\n",
      "If he would have mirs the weary more\n"
     ]
    }
   ],
   "source": [
    "print(generated_outputs.numpy()[0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\n",
      "At wry bids and vow: and Lucio,\n",
      "Whether I will in true black the Iam wretch.\n",
      "O Lord of Trust\n",
      "of little Master Daughter\n",
      "That thus Earl of York, the duke's bosom of Critizens:\n",
      "What you, I say, remember your father?\n",
      "Therefore no doubt a dame uncle, you may shake\n",
      "And yet with you, I'll be the royal prevented's\n",
      "The twomen of my chances feasting of blesh.\n",
      "What the timer of the days of Nathant's swift?\n",
      "I'll withur love as for comper; the capefore ashius\n",
      "obey followers goass of the second which should\n"
     ]
    }
   ],
   "source": [
    "print(generated_outputs.numpy()[1].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\n",
      "The ground: that's in the night of the music that's\n",
      "else slipe; these comes of the curness is\n",
      "Will not perceive it in the heavy tlouckle,\n",
      "To mine executioners, the worms of such as kindred.\n",
      "Fearing striket a fellow honour of the rude,\n",
      "Go to they are and gotten Claudio; and therefore,\n",
      "Incle, as thou hast not still be rack:\n",
      "God's them heavy offlicts our patch'd part\n",
      "With all patient she winks, and pleasesure you\n",
      "Fair acted from me your wears welcome.\n",
      "Now, beseech you may not, more walkon.\n",
      "\n",
      "CLARE\n"
     ]
    }
   ],
   "source": [
    "print(generated_outputs.numpy()[2].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RICHARD:\n",
      "Here wench in Brespare, quarter, by are the telling!\n",
      "\n",
      "PARIS:\n",
      "The son the duest of telling imour tree, and thou only\n",
      "fair mercy before than of thy world. They can in home.\n",
      "\n",
      "LUCIO:\n",
      "\n",
      "GRUMIO:\n",
      "Great Sanday, be a knee purpose: if fill not the bell,\n",
      "nothing pat we have commended him lived and so\n",
      "shuccess at should have ase done, have I,\n",
      "Have 'pearls;' forget, can you love Just me to cause the trumpet!\n",
      "And, after you have plain'd with her service,\n",
      "Which you are, mother mouth: Eter of the night\n"
     ]
    }
   ],
   "source": [
    "print(generated_outputs.numpy()[3].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if tr.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# hyperparameters\n",
    "tr_batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "tr_block_size = 256 # what is the maximum context length for predictions?\n",
    "\n",
    "tr_max_iters = 5000\n",
    "tr_eval_interval = 500\n",
    "tr_learning_rate = 3e-4\n",
    "tr_eval_iters = 200\n",
    "\n",
    "tr_n_layer = 6\n",
    "tr_drop_out_rate = 0.2\n",
    "\n",
    "tr_ffn_units = 32\n",
    "tr_n_embed = 384\n",
    "tr_n_heads = 6\n",
    "tr_head_dims = tr_n_embed // tr_n_heads  \n",
    "\n",
    "\n",
    "assert(tr_head_dims * tr_n_heads == tr_n_embed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a mapping from characters to integers and vice-versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers and vice-versa\n",
    "\n",
    "tr_vocab_size = len(all_chars)\n",
    "\n",
    "c_to_i = {c:i for i,c in enumerate(all_chars)}\n",
    "i_to_c = {i:c for i,c in enumerate(all_chars)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tr_str_encode = lambda s : [c_to_i[c] for c in s ]\n",
    "tr_str_decode = lambda id : ''.join([i_to_c[i] for i in id])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data = tr.tensor(tr_str_encode(text_file), dtype=tr.long)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's now split up the data into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 90% will be train, rest val\n",
    "n = int(0.9*len(tr_data)) \n",
    "tr_train_data = tr_data[:n]\n",
    "tr_val_data = tr_data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.manual_seed(1337)\n",
    "# how many independent sequences will we process in parallel?\n",
    "# tr_batch_size = 32 \n",
    "# what is the maximum context length for predictions?\n",
    "# tr_block_size = 101 \n",
    "\n",
    "def get_batch(split='train', batch_size=tr_batch_size, block_size=tr_block_size):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = tr_train_data if split == 'train' else tr_val_data\n",
    "    ix = tr.randint(len(data) - block_size, (batch_size,))\n",
    "    x = tr.stack([data[i:i+block_size] for i in ix])\n",
    "    y = tr.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    # if device is cuda -- \n",
    "        # when we load the data , move it to the device\n",
    "    x, y = x.to(device), y.to(device) \n",
    "    return x, y\n",
    "\n",
    "@tr.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'eval']:\n",
    "        losses = tr.zeros(tr_eval_iters)\n",
    "\n",
    "        for k in range(tr_eval_iters):\n",
    "            X, Y = get_batch(split=split, batch_size=tr_batch_size, block_size=tr_block_size)\n",
    "            logits, loss = model(inputs=X, targets=Y)\n",
    "            losses[k] = loss\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train', batch_size=4, block_size=8)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "# for b in range(batch_size): # batch dimension\n",
    "#     for t in range(block_size): # time dimension\n",
    "#         context = xb[b, :t+1]\n",
    "#         target = yb[b,t]\n",
    "#         print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(tr.nn.Module):\n",
    "    def __init__(self, embedding_dim, head_dims, block_size, drop_out_rate):\n",
    "        super(Head, self).__init__()\n",
    "        self.keys = tr.nn.Linear(in_features=embedding_dim, out_features=head_dims, bias=False)\n",
    "        self.query = tr.nn.Linear(in_features=embedding_dim, out_features=head_dims, bias=False)\n",
    "        self.values = tr.nn.Linear(in_features=embedding_dim, out_features=head_dims, bias=False)\n",
    "        # This is typically used to register a buffer that should not to be considered a model parameter. \n",
    "        # For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers,\n",
    "        # by default, are persistent and will be saved alongside parameters.\n",
    "        self.register_buffer(name='lt_matrix', tensor=tr.tril(tr.ones(block_size, block_size)))\n",
    "        self.dropout = tr.nn.Dropout(drop_out_rate)\n",
    "\n",
    "\n",
    "    def self_attention(self, key, query, values):\n",
    "        B, T, C = key.shape\n",
    "        # [batch, time, head_dims] @ [batch, head_dims, time] --> [batch, time, time]\n",
    "        dot_product =  key @ tr.permute(query, dims=[0, 2, 1])\n",
    "        mask = self.lt_matrix[:T, :T] # causality constraints\n",
    "        dot_product = tr.masked_fill(input=dot_product, mask=mask==0, value=-tr.inf)\n",
    "        scaled_dot_product = dot_product * C**-0.5\n",
    "        weights = tr.nn.functional.softmax(scaled_dot_product, dim=-1)\n",
    "        # [batch, time, time] @ [batch, time, head_dims] -- > [batch, time, head_dims]\n",
    "        attention = weights @ values\n",
    "        return attention\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # [batch, time, embedding_dims] --> [batch, time, head_dims]\n",
    "        k = self.keys(inputs)\n",
    "        q = self.query(inputs)\n",
    "        v = self.values(inputs)\n",
    "\n",
    "        attention = self.self_attention(k, q, v) # -- > [batch, time, head_dims]\n",
    "\n",
    "        return attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tr.nn.Module):\n",
    "    \"\"\"Multiple heads of self attention in parallel\"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, head_dims, block_size, drop_out_rate):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.multi_heads = tr.nn.ModuleList(\n",
    "            [Head(embedding_dim, head_dims, block_size, drop_out_rate) for i in range(num_heads)])\n",
    "        self.linear = tr.nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
    "        self.dropout = tr.nn.Dropout(drop_out_rate)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # eg: a list containing 4 heads of 8 dims self attention  \n",
    "        # [batch, time, embedding_dim] --> [ .. [batch, time, head_dims] ..  ]\n",
    "        mha = [head(inputs) for head in self.multi_heads]\n",
    "        # there are num_head self attention output so we concat along the last axis\n",
    "        out = tr.cat(tensors=mha, dim=-1)  # --> [batch, time, embedding_dims]\n",
    "        out = self.linear(out)\n",
    "        return self.dropout(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(tr.nn.Module):\n",
    "\n",
    "    def __init__(self, ffn_units, n_embed, drop_out_rate):\n",
    "        super(FFNN, self).__init__()\n",
    "        \n",
    "        self.ffn = tr.nn.Sequential(\n",
    "            tr.nn.Linear(in_features=n_embed, out_features=4*ffn_units),\n",
    "            tr.nn.ReLU(),\n",
    "            tr.nn.Linear(in_features=4*ffn_units, out_features=n_embed),\n",
    "            tr.nn.Dropout(drop_out_rate)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.ffn(inputs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(tr.nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, head_dims, block_size, ffn_units, drop_out_rate) -> None:\n",
    "        super(Block, self).__init__()\n",
    "        self.mha = MultiHeadAttention(embedding_dim, num_heads, head_dims, block_size, drop_out_rate)\n",
    "        self.ffnn = FFNN(ffn_units, embedding_dim, drop_out_rate)\n",
    "        self.layer_norm_1 = tr.nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        self.layer_norm_2 = tr.nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        assert(tr_n_heads*tr_head_dims == tr_n_embed)\n",
    "        \n",
    "        #  embedding_dim = heads * head_dims\n",
    "        # multi head attention block --> [batch, time, embedding_dim]\n",
    "        ln1 = self.layer_norm_1(inputs) # pre-norm # Layer normalization\n",
    "        x = inputs + self.mha(ln1)  # skip connection + Multi-head-attention\n",
    "\n",
    "        # Feed forward neural network \n",
    "        ln2 = self.layer_norm_2(x) # pre-norm # Layer normalization\n",
    "        x = x + self.ffnn(ln2) # skip connection + FFNN --> [batch, time, embedding_dims]\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(tr_n_heads*tr_head_dims == tr_n_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input sequence torch.Size([32, 8])\n",
      "Shape of output logits torch.Size([256, 65])\n",
      "Output loss tensor(4.4517, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(tr.nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embedding_dim, num_heads, head_dims, ffn_units, n_layer, drop_out_rate):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.token_embedding_table = tr.nn.Embedding(num_embeddings=vocab_size, embedding_dim = embedding_dim)\n",
    "        self.position_embedding_table = tr.nn.Embedding(num_embeddings=block_size, embedding_dim = embedding_dim)\n",
    "        self.linear = tr.nn.Linear(in_features=embedding_dim, out_features=vocab_size)\n",
    "        self.layer_norm = tr.nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "\n",
    "        self.mha_ffnn_block = tr.nn.Sequential(\n",
    "            *[Block(embedding_dim, num_heads, head_dims, block_size, ffn_units, drop_out_rate) for _ in range(n_layer)])\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, inputs, targets=None):\n",
    "        (B, T) = inputs.shape\n",
    "        # [T] --> [T, embedding_dim]\n",
    "        pe = self.position_embedding_table(tr.arange(end=T, device=device))\n",
    "        # [batch, timesteps] --> [batch, time, embedding_dim]\n",
    "        embed = self.token_embedding_table(inputs)\n",
    " \n",
    "        # [batch, time, embedding_dim] + [time, embedding_dim] --> [batch, time, embedding_dim]\n",
    "        x = embed + pe \n",
    "\n",
    "\n",
    "        #  embedding_dim = heads * head_dims\n",
    "        # multi head attention block --> [batch, time, embedding_dim]\n",
    "        # Feed forward neural network # -->  [batch, time, embedding_dim]\n",
    "        x = self.mha_ffnn_block(x)\n",
    "\n",
    "        # layer normalization\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        # [batch, time, embedding_dim] --> [batch, time, vocab_size]\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # reshaping \n",
    "            (batch, time, embedding_dim) = logits.shape\n",
    "            logits = logits.view(batch*time, embedding_dim)\n",
    "            targets = targets.view(-1)\n",
    "\n",
    "            loss = tr.nn.functional.cross_entropy(input=logits, target=targets)\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block-size tokens , because PE expect in_dims = block_size\n",
    "            idx_in = idx[:,-self.block_size:]\n",
    "            # get the predictions\n",
    "            (logits, loss) = self(idx_in)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = tr.nn.functional.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = tr.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = tr.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "    \n",
    "\n",
    "tr_model = BigramLanguageModel(\n",
    "    tr_vocab_size, tr_block_size, tr_n_embed, tr_n_heads, tr_head_dims, tr_ffn_units, tr_n_layer, tr_drop_out_rate\n",
    ")\n",
    "# when we create the model, we want to move the model params to the device\n",
    "tr_model = tr_model.to(device)\n",
    "tr_sample_logits, tr_sample_loss = tr_model(xb, yb)\n",
    "\n",
    "\n",
    "print('Shape of input sequence', xb.shape)\n",
    "print('Shape of output logits', tr_sample_logits.shape)\n",
    "print('Output loss', tr_sample_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets test generating some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 22, 41, 47, 64, 57, 45, 28,  0, 44, 17, 12, 51, 54, 57, 60, 17, 36,\n",
       "         47, 56, 17, 13, 31, 57, 26, 37,  0, 52, 31, 16, 38, 35, 30, 44, 62, 50,\n",
       "         32, 26, 34, 11, 22, 50, 53,  4, 12, 63,  2, 16, 14, 12, 15, 62, 62, 52,\n",
       "         39, 22, 31, 36, 41, 43, 37, 57, 40, 44, 22, 20, 53, 33, 24,  5, 33, 12,\n",
       "         27, 10, 36,  8, 47, 36, 46, 26, 34, 52, 49, 28, 47, 53, 30, 63, 48, 18,\n",
       "         45, 39, 24, 27, 40, 32,  3, 41, 14, 51, 14]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure to add the context to the device\n",
    "context = tr.zeros(size=(1,1),dtype=tr.long, device=device)\n",
    "tr_model.generate(context, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_model.generate(tr.zeros(size=(1,1), dtype=tr.long), max_new_tokens=100).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IN Nock ter cill rine kinsoutsme wiy, os.\n",
      "\n",
      "VIE:\n",
      "O\n",
      "Tosned;\n",
      "Coond;\n",
      "I bone the fora Gomase.\n",
      "\n",
      "BOMEIZIUS:\n"
     ]
    }
   ],
   "source": [
    "print(tr_str_decode(tr_model.generate(tr.zeros(size=(1,1), dtype=tr.long), max_new_tokens=100).squeeze().tolist()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a pytorch optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tr.optim.AdamW(params=tr_model.parameters(), lr=tr_learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training:   0%|\u001b[32m                                            \u001b[0m| 1/5000 [00:04<5:50:07,  4.20s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 4.51 Validation loss 4.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training:   4%|\u001b[32m█▊                                          \u001b[0m| 203/5000 [00:14<24:03,  3.32it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.60 Validation loss 2.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training:   8%|\u001b[32m███▌                                        \u001b[0m| 403/5000 [00:24<25:41,  2.98it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.45 Validation loss 2.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training:  12%|\u001b[32m█████▎                                      \u001b[0m| 605/5000 [00:34<17:59,  4.07it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.34 Validation loss 2.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training:  16%|\u001b[32m███████                                     \u001b[0m| 803/5000 [00:44<21:13,  3.30it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.30 Validation loss 2.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training:  20%|\u001b[32m████████▋                                  \u001b[0m| 1005/5000 [00:54<17:14,  3.86it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.27 Validation loss 2.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training:  24%|\u001b[32m██████████▎                                \u001b[0m| 1203/5000 [01:04<19:35,  3.23it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.22 Validation loss 2.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training:  28%|\u001b[32m████████████                               \u001b[0m| 1402/5000 [01:14<17:52,  3.35it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.21 Validation loss 2.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training:  32%|\u001b[32m█████████████▊                             \u001b[0m| 1605/5000 [01:24<12:33,  4.50it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.18 Validation loss 2.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training:  36%|\u001b[32m███████████████▌                           \u001b[0m| 1805/5000 [01:34<12:10,  4.37it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.15 Validation loss 2.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training:  40%|\u001b[32m█████████████████▏                         \u001b[0m| 2005/5000 [01:44<11:19,  4.41it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.13 Validation loss 2.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training:  44%|\u001b[32m██████████████████▉                        \u001b[0m| 2203/5000 [01:53<13:39,  3.41it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.12 Validation loss 2.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training:  48%|\u001b[32m████████████████████▋                      \u001b[0m| 2405/5000 [02:03<09:20,  4.63it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.11 Validation loss 2.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training:  52%|\u001b[32m██████████████████████▍                    \u001b[0m| 2605/5000 [02:13<08:45,  4.56it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.10 Validation loss 2.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training:  56%|\u001b[32m████████████████████████                   \u001b[0m| 2803/5000 [02:23<10:36,  3.45it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.07 Validation loss 2.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training:  60%|\u001b[32m█████████████████████████▊                 \u001b[0m| 3004/5000 [02:32<09:43,  3.42it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.07 Validation loss 2.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training:  64%|\u001b[32m███████████████████████████▌               \u001b[0m| 3203/5000 [02:42<08:38,  3.47it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.06 Validation loss 2.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training:  68%|\u001b[32m█████████████████████████████▎             \u001b[0m| 3405/5000 [02:52<05:42,  4.66it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.06 Validation loss 2.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training:  72%|\u001b[32m██████████████████████████████▉            \u001b[0m| 3604/5000 [03:01<06:58,  3.33it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.05 Validation loss 2.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training:  76%|\u001b[32m████████████████████████████████▋          \u001b[0m| 3806/5000 [03:11<04:16,  4.66it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.04 Validation loss 2.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training:  80%|\u001b[32m██████████████████████████████████▍        \u001b[0m| 4006/5000 [03:21<03:29,  4.75it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.03 Validation loss 2.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training:  84%|\u001b[32m████████████████████████████████████▏      \u001b[0m| 4204/5000 [03:30<03:54,  3.39it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.02 Validation loss 2.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training:  88%|\u001b[32m█████████████████████████████████████▉     \u001b[0m| 4406/5000 [03:40<02:06,  4.70it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.00 Validation loss 2.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training:  92%|\u001b[32m███████████████████████████████████████▌   \u001b[0m| 4603/5000 [03:50<02:06,  3.14it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.01 Validation loss 2.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training:  96%|\u001b[32m█████████████████████████████████████████▎ \u001b[0m| 4802/5000 [03:59<00:58,  3.38it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2.01 Validation loss 2.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training: 100%|\u001b[32m███████████████████████████████████████████\u001b[0m| 5000/5000 [04:05<00:00, 20.35it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in trange(tr_max_iters, ncols=100, colour='green', desc='Model training'):\n",
    "\n",
    "    if i % tr_eval_iters == 0:\n",
    "        losses = estimate_loss(model=tr_model)\n",
    "        print(f'Train loss { losses[\"train\"] :0.2f} Validation loss {losses[\"eval\"] :0.2f}')\n",
    "\n",
    "    # get the data\n",
    "    xb, yb = get_batch(split='train', batch_size=tr_batch_size, block_size=tr_block_size)\n",
    "    # fit the model\n",
    "    logits, loss = tr_model(inputs=xb, targets=yb)\n",
    "    # zero out gradients from previous step\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # get the gradients for all the model params\n",
    "    loss.backward()\n",
    "    # use the gradients to update the params\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.069420099258423"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TRLA:\n",
      "Your my conver have, and lije, I me in in it hay,\n",
      "Und dumnepes Weoul the:\n",
      "Waled, and of dear here hath beer uls, you my vee meme's me rither sitely batce,\n",
      "I rale house of itue are crithioer, tul\n"
     ]
    }
   ],
   "source": [
    "print(tr_str_decode(tr_model.generate(tr.zeros(size=(1,tr_block_size), dtype=tr.long), max_new_tokens=200).squeeze().tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
